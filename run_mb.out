
Load Dataset
label column: label
label dictionary: {'normal': 0, 'tumor': 1}
number of classes: 2
slide-level counts:  
 label
0    238
1    160
Name: count, dtype: int64
Patient-LVL; Number of samples registered in class 0: 238
Slide-LVL; Number of samples registered in class 0: 238
Patient-LVL; Number of samples registered in class 1: 160
Slide-LVL; Number of samples registered in class 1: 160
split_dir:  splits/task_1_tumor_vs_normal_100
################# Settings ###################
num_splits:  10
k_start:  -1
k_end:  -1
task:  task_1_tumor_vs_normal
max_epochs:  200
results_dir:  ./results
lr:  0.0002
experiment:  task_1_tumor_vs_normal_CLAM_100
reg:  1e-05
label_frac:  1.0
bag_loss:  ce
seed:  1
model_type:  clam_mb
model_size:  small
use_drop_out:  0.25
weighted_sample:  True
opt:  adam
bag_weight:  0.7
inst_loss:  svm
B:  8
split_dir:  splits/task_1_tumor_vs_normal_100

Training Fold 0!

Init train/val/test splits... 
Done!
Training on 318 samples
Validating on 40 samples
Testing on 40 samples

Init loss function... Done!

Init Model... Setting tau to 1.0
Done!
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=2, bias=True)
    )
  )
  (classifiers): Linear(in_features=512, out_features=2, bias=True)
  (instance_classifiers): ModuleList(
    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): SmoothTop1SVM()
)
Total number of parameters: 791048
Total number of trainable parameters: 791048

Init optimizer ... Done!

Init Loaders... Done!

Setup EarlyStopping... Done!


tensor([[0],
        [0]], device='cuda:0')
